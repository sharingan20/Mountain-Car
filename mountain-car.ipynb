{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mountain-car.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7uy2rwBzaP8Zhb43tkeTg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IE6LroLITlhg"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install colabgymrender==1.0.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eg69Vvv0TrBG"},"source":["! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sqkg9NfuTtxZ"},"source":["import gym\n","from colabgymrender.recorder import Recorder\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8cIskkVcyrcb","executionInfo":{"status":"ok","timestamp":1633508703943,"user_tz":-330,"elapsed":358,"user":{"displayName":"Rishabh D A","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08186196616710850135"}},"outputId":"f7ef62da-fe8b-4e21-912a-93f3b2105809"},"source":["env = gym.make(\"MountainCar-v0\")\n","env._max_episode_steps = 1000\n","#####saved model parameters#####\n","# 48,24,3\n","#adam(lr=0.001)  mse\n","\n","def create_model():\n","  inputs = layers.Input(shape=(2))# - actual used for saved mode\n","  layer1 = layers.Dense(48,activation='relu')(inputs)#\n","  layer2 = layers.Dense(24,activation='relu')(layer1)\n","  action = layers.Dense(3)(layer2)\n","\n","  return keras.Model(inputs=inputs, outputs=action)\n","\n","model = create_model()\n","model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss=\"mean_squared_error\")\n","target_model = create_model()\n","target_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),loss=\"mean_squared_error\")\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 2)]               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 48)                144       \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 24)                1176      \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 3)                 75        \n","=================================================================\n","Total params: 1,395\n","Trainable params: 1,395\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"feaTPdUA3Nkd"},"source":["\n","total_rewards = 0\n","frame_count = 0\n","episode_count = 0\n","max_memory_length = 100000\n","update_after = 4\n","update_target_after = 1000\n","epsilon = 1\n","gamma = 0.99\n","\n","action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","flag=0\n","for i in range(1000):\n","  state = env.reset()\n","  state = np.array(state)\n","  rewards=0\n","\n","  for step in range(1,1000):\n","    frame_count+=1\n","\n","    if frame_count < 30000 or epsilon > np.random.rand(1)[0]:\n","      action = np.random.choice(3)\n","    else:\n","      state_tensor = tf.convert_to_tensor(state)\n","      state_tensor = tf.expand_dims(state_tensor,0)\n","      action_prob = model.predict(state_tensor)\n","      action = tf.argmax(action_prob[0]).numpy()\n","\n","    epsilon -= 0.99/100000\n","    epsilon = max(epsilon,0.1)\n","\n","    state_next,reward,done,info = env.step(action)\n","    state_next = np.array(state_next)\n","\n","    rewards += reward\n","\n","    action_history.append(action)\n","    state_history.append(state)\n","    state_next_history.append(state_next)\n","    done_history.append(done)\n","    rewards_history.append(reward)\n","\n","    state = state_next\n","\n","    if len(done_history) > 32:\n","      indices = np.random.choice(range(len(done_history)),size=32)\n","\n","      state_sample = np.array([state_history[i] for i in indices])\n","      state_next_sample = np.array([state_next_history[i] for i in indices])\n","      rewards_sample = [rewards_history[i] for i in indices]\n","      action_sample = [action_history[i] for i in indices]\n","      done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n","\n","      future_rewards = target_model.predict(state_next_sample)\n","      current_rewards = model.predict(state_sample)\n","      updated_q = current_rewards[:]\n","\n","      for idx,terminal in enumerate(done_sample):\n","        if terminal:\n","          future_rewards[idx]=0.0\n","        updated_q[idx,action_sample[idx]] = rewards_sample[idx] + gamma*tf.reduce_max(future_rewards,axis=1)[idx]\n","\n","      model.train_on_batch(state_sample,updated_q)\n","\n","    if frame_count % update_target_after ==0:\n","      target_model.set_weights(model.get_weights())\n","      #print(\"reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n","\n","    if len(rewards_history)>max_memory_length:\n","      del rewards_history[:1]\n","      del state_history[:1]\n","      del state_next_history[:1]\n","      del action_history[:1]\n","      del done_history[:1]\n","\n","    if done:\n","      break\n","\n","  print(\"episode reward:\",rewards,\"mean reward\",total_rewards,\"episode:\",episode_count,\"frame:\",frame_count)\n","  episode_reward_history.append(rewards)\n","  if len(episode_reward_history)>100:\n","    del episode_reward_history[:1]\n","  episode_count+=1\n","  total_rewards = np.mean(episode_reward_history)\n","\n","  if episode_count%100==0:\n","    model.save('/content/gdrive/My Drive/Reinforcement-Learning-project/mountaincar-{}.h5'.format(episode_count))\n","    model.save('mountaincar-{}.h5'.format(episode_count))\n","\n","  if total_rewards > -110:\n","    print(\"solved at episode: \",episode_count)\n","    break\n","#env.play()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KdfEwLUEv7JY"},"source":["directory = './video'\n","env = Recorder(env, directory)\n","actions = []\n","reward = []\n","for h in range(10):\n","  done=False\n","  obs=env.reset()\n","  obs=np.array(obs)\n","  reward=0\n","  while not done:\n","    obs = tf.convert_to_tensor(obs)\n","    obs = tf.expand_dims(obs,0)\n","    action = model.predict(obs)\n","    a = tf.argmax(action[0]).numpy()\n","    obs_,r,done,_ = env.step(a)\n","    obs=np.array(obs_)\n","    actions.append(a)\n","    reward+=r\n","  print(reward)\n","#env.play()"],"execution_count":null,"outputs":[]}]}